# Un recorrido por mis proyectos de Data Engineering 

隆Hola! Soy una apasionada del **data engineering** en un viaje de aprendizaje continuo, trabajando en pipelines escalables, modulares y optimizados para big data. Este repositorio es mi **ecosistema** de proyectos, donde documento mi progreso bajo la gu铆a de mi **profe**.

## Sobre m铆 y mi aprendizaje

Mi objetivo es construir una base s贸lida en **data engineering** con un enfoque en **escalabilidad**, **optimizaci贸n de recursos**, y **flujos modulares**. 

A trav茅s de una serie de proyectos pr谩cticos, he avanzado desde los fundamentos hasta pipelines integrando herramientas como:

- **Polars**: Procesamiento ultrarr谩pido con expresiones lazy nativas (`rolling`, `shift`, `when`).
- **Dask**: Paralelismo distribuido para datasets &gt; RAM (aun en proceso).
- **PyArrow**: Escritura eficiente en Parquet con zero-copy.
- **TOML/Pydantic**: Configuraciones din谩micas con validaciones avanzadas (rangos, unicidad, regex).
- **Prefect**: Orquestaci贸n de flujos con reintentos y modularidad.
- **Error handling**: Gesti贸n de edge cases (nulos, columnas faltantes, datasets vac铆os).

## Enfoque Metodol贸gico

 Cada proyecto est谩 dise帽ado bajo los siguientes principios:

- **Escalabilidad**: Pipelines que tratan de manejan datasets grandes.
- **Modularidad**: C贸digo organizado (OOP, SOLID) para reutilizaci贸n y mantenimiento.
- **Optimizaci贸n**: Lazy evaluation, streaming, y PyArrow para minimizar recursos.
- **Vida real**: Enfoque con configs en TOML/YAML, validaciones robustas, y orquestaci贸n
- **Aprendizaje continuo**: Cada commit es un paso hacia ser una data engineer, con un ojo en ML. 

## Tecnolog铆as y herramientas

- **Python**: Base de todos los pipelines.
- **Polars**: Procesamiento lazy para big data (en proceso)
- **Dask**: Paralelismo distribuido.
- **PyArrow**: Escritura eficiente en Parquet.
- **Pydantic**: Validaciones avanzadas para configs.
- **TOML**: Configuraciones legibles y din谩micas.
- **Prefect**: Orquestaci贸n de flujos.
- **GitHub**: Control de versiones y colaboraci贸n.

## Pr贸ximos pasos

- Dominar **Ray + Polars + PyArrow** con pipelines m谩s complejos (e.g., joins distribuidos, window functions avanzadas).
- Integrar **scikit-learn** o **PyTorch** para ML engineering, usando mis pipelines para feature engineering.
- Explorar **OpenTelemetry** para monitorear RAM/CPU en producci贸n.

## Agradecimientos

Gracias por los ejemplos pr谩cticos, las revisiones detalladas, y por empujarme a optimizar recursos a Grok.ai. 隆Seguimos trabajando!  

---

隆Gracias por visitar mi repo! Si quieres colaborar o dar feedback, estoy abierta a ideas. 