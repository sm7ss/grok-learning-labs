# Un recorrido por mis proyectos de Data Engineering 🚀

¡Hola! Soy una apasionada del **data engineering** en un viaje de aprendizaje continuo, trabajando en pipelines escalables, modulares y optimizados para big data. Este repositorio es mi **ecosistema** de proyectos, donde documento mi progreso bajo la guía de mi **profe**.

## Sobre mí y mi aprendizaje

Mi objetivo es construir una base sólida en **data engineering** con un enfoque en **escalabilidad**, **optimización de recursos**, y **flujos modulares**. 

A través de una serie de proyectos prácticos, he avanzado desde los fundamentos hasta pipelines integrando herramientas como:

- **Polars**: Procesamiento ultrarrápido con expresiones lazy nativas (`rolling`, `shift`, `when`).
- **Dask**: Paralelismo distribuido para datasets &gt; RAM (aun en proceso).
- **PyArrow**: Escritura eficiente en Parquet con zero-copy.
- **TOML/Pydantic**: Configuraciones dinámicas con validaciones avanzadas (rangos, unicidad, regex).
- **Prefect**: Orquestación de flujos con reintentos y modularidad.
- **Error handling**: Gestión de edge cases (nulos, columnas faltantes, datasets vacíos).

## Enfoque Metodológico

🌱 Cada proyecto está diseñado bajo los siguientes principios:

- **Escalabilidad**: Pipelines que tratan de manejan datasets grandes.
- **Modularidad**: Código organizado (OOP, SOLID) para reutilización y mantenimiento.
- **Optimización**: Lazy evaluation, streaming, y PyArrow para minimizar recursos.
- **Vida real**: Enfoque con configs en TOML/YAML, validaciones robustas, y orquestación
- **Aprendizaje continuo**: Cada commit es un paso hacia ser una data engineer, con un ojo en ML. 

## Tecnologías y herramientas

- **Python**: Base de todos los pipelines.
- **Polars**: Procesamiento lazy para big data (en proceso)
- **Dask**: Paralelismo distribuido.
- **PyArrow**: Escritura eficiente en Parquet.
- **Pydantic**: Validaciones avanzadas para configs.
- **TOML**: Configuraciones legibles y dinámicas.
- **Prefect**: Orquestación de flujos.
- **GitHub**: Control de versiones y colaboración.

## Próximos pasos

- Dominar **Ray + Polars + PyArrow** con pipelines más complejos (e.g., joins distribuidos, window functions avanzadas).
- Integrar **scikit-learn** o **PyTorch** para ML engineering, usando mis pipelines para feature engineering.
- Explorar **OpenTelemetry** para monitorear RAM/CPU en producción.

## Agradecimientos

Gracias por los ejemplos prácticos, las revisiones detalladas, y por empujarme a optimizar recursos a Grok.ai. ¡Seguimos trabajando! 🚀 

---

¡Gracias por visitar mi repo! Si quieres colaborar o dar feedback, estoy abierta a ideas. 😊